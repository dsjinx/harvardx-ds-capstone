---
title: "CYOincome"
author: "Jin Xin"
output: pdf_document
date: "Oct, 2022"
bibliography: CYOref.bib
---

```{r include = FALSE}
#these pkgs used for generating report
library(tidyverse)
library(data.table)
library(caret)
library(doParallel)
library(e1071)
library(caretEnsemble)
library(GGally)
```
```{r eval = FALSE}
#libraries are used in this report
library(tidyverse)
library(data.table)
library(caret)
library(doParallel)
library(e1071)
library(caretEnsemble)
library(GGally)
```
##Executive Summary
This report is going to apply different methods for achieving a good $F_1$ score on the target income class out of all the survey candidates. The used Adult Income Census dataset has two income classes: "<= 50K", ">50K". The goal is to successfully predict all the ">50K" income samples in the testset. But the challenge is the class dominance of "<=50K", which made the ">50K" class to be short-sampled, so it is hard to achieve a good accuracy of ">50K" class. The methods used in this report include CART (classification and regression tree), random forest, SVM (support vector machine), and logistic regression. The best performing individual method is ??? with a $F_1$ score of ???. In addition, the stacking method is used to combine the tuned random forest, SVM, and GLM, and the achieved $F_1$ score is ????. The results indicate that the .... is the best performing method in this dateset.

##Introduction
We are going to apply different but not exhaustive classification methods in the report. The $F_1$ score is used as evaluation metric, the reason is coming from the unbalanced sample size in the dataset. The Adult Income Census datast we use in this report classifies the income levels into two groups, one is "<=50K", and the other ">50K", but the ">50K" class has much smaller sample size comparing to "<=50" class, [@nielsen_2015] and many practical machine learning articles suggest that better result can always be achieved by increasing sample size. On the contrary, small sample size may hinder to achieve a desirable result. So the balanced $F_1$ score can help us evaluate both sensitivity and specificity on the target ">50K" class, this attribute is making $F_1$ a rather effective metric comparing overall accuracy, which will be showing a false high value due to the big sample size of the other class.  

The following content is broken into three sections. In the data exploration part, we will explore the data in a top-down strategy, by having a broad view of the dataset structure, and break the predictor attributes into groups according to the value classes, then look for any trend or relations across those attributes. After that, we will go into methods details, we will justify the reasons for choosing those methods used in this report. Lastly, we will evaluate the overall performances across different methods. 

##Data Exploration
The [Adult Census Income](www.kaggle.com/datasets/uciml/adult-census-income) dataset we use is from the 1994 Census bureau database. This dataset has the dimension of 32561 by 15, which has samples in rows, and measured variables in columns, so every single sample has 15 different attributes, and there are 32561 samples in total. 
```{r echo  = FALSE}
str(data)
```
The attributes are measured both quantitatively and qualitatively with respect to integer class and character class. The dependent variable "income" is what we are trying to predict, and it is in a qualitative form with only two different categories: "<=50K" and ">50K". This two classes element structure can help us narrow down the possible methods used will be either producing "YES" or "NO" results or giving the probability toward each category.
```{r echo  = FALSE}
sapply(data, function(j) class(j))
```

###Data Cleaning
Before we explore the data in further details, we need to do some housekeeping on the data to clean out all the strange and missing values. 

Although there is no "NA", but we can see some "?" showing in "workclass", "occupation", and "native.country". Since they are all in the qualitative attributes, and recorded in character string, so we will decide the way to handle them, when we explore these attributes in details. 
```{r}
sum(is.na(data))
```
```{r echo = FALSE}
temp <- data[, lapply(.SD, function(j) sum(str_detect(j, "\\?")))]
data.table(vars = names(temp),
           "num of ?" = as.numeric(temp)) %>% knitr::kable()
rm(temp)
```

###Data Visualisation
After the inital housekeeping on the dateset, we now get into more details about the dependent variable "income", and the other 14 attributes as predictors. For the "income", samples are classified into two categories: "<=50K" and ">50K". By checking the distribution across those two income levels we can see that, the income of "<=50K" is the dominant group across all the samples with the size being 3 times of the ">50K" group, and only about 24% of the sample belongs to ">50K". 
```{r}
sum(data$income == "<=50K") / sum(data$income == ">50K")
```
```{r echp = FALSE}
data <- data[, income := as.factor(income)]
plot_temp <- data[, .(pct = 100 * .N / dim(data)[1]), by = .(income)]
ggplot(plot_temp, aes(income, pct)) + 
  geom_bar(stat = "identity", width = 0.3) + 
  geom_text(aes(label = format(round(pct, 2), nsmall = 2)),
            vjust = 2, color = "white")

rm(plot_temp)
```
This unbalanced sample size can potentially make a high accuracy on predicting ">50K" income group to be more challenging than the "<=50K" ones, the reason is that we have more samples to study on with the "<=50K" group, but for under-sampled ">50K" group, if their attributes are not clustered, or lack of strong manifestation on their 14 attributes, then we can only reply on minimising the error on "<=50K" group to improve the ">50K" group accuracy. On the other hand, this can be viewed as an advantage of having only two groups of dependents. 

As mentioned in above, there are two different classes of predictors: "integer" and "character", so we will look those two classes of attributes separately. We will look into the "integer" class first. 
```{r}
cols <- names(which(sapply(data, class) == "integer"))
summary(data[, ..cols]) #check any strange value
```
The above lot of "integer" columns summary is not showing any strange numbers, except in the "capital.gain" and "capital.loss" columns are 0's for most of their statistics, it makes sense that most people are earning less than 50K/year, and most of them won't have investment asset in place. Apart from this, the column "fnlwgt" we need to highlight about its meaning in place, it is stand for "final weight", simply speaking it represents the amount of people is sharing the same attributes of the row, which that "fnlwgt" value is in. Of course the census bureau has more rigorous explanation on it, but we just want to highlight its meaning to avoid the ambiguity caused by its name having a "weight" in it. 
```{r}
num_fp <- data[, ..cols][, lapply(.SD, 
              function(j) (j - mean(j)) / sd(j)), .SDcols = cols]
summary(num_fp) #check potential outliers by normalising the columns
```
After we normalise all the "integer" columns, it can give us much clearer picture about how far away those outliers are. Next we will look into the relations inbetween those numeric attributes under differnt income groups. 

```{r echo = FALSE}

```