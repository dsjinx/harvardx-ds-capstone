---
title: "CYOincome"
author: "Jin Xin"
output: pdf_document
date: "Oct, 2022"
bibliography: CYOref.bib
---

```{r include = FALSE}
#these pkgs used for generating report
library(tidyverse)
library(data.table)
library(caret)
library(doParallel)
library(e1071)
library(caretEnsemble)
library(GGally)

```
```{r eval = FALSE}
#libraries are used in this report
library(tidyverse)
library(data.table)
library(caret)
library(doParallel)
library(e1071)
library(caretEnsemble)
library(GGally)
```
##Executive Summary
This report is going to apply different methods for achieving a good $F_1$ score on the target income class out of all the surveyed candidates. The used Adult Income Census dataset has two income classes: "<= 50K", ">50K". The goal is to successfully predict all the ">50K" income samples in the testset. But the challenge is the class dominance of "<=50K", which made the ">50K" class to be short-sampled, so it is hard to achieve a good accuracy of ">50K" class. The methods used in this report include CART (classification and regression tree), random forest, SVM (support vector machine), and logistic regression. The best performing individual method is ??? with a $F_1$ score of ???. In addition, the stacking method is used to combine the tuned random forest, SVM, and GLM, and the achieved $F_1$ score is ????. The results indicate that the .... is the best performing method in this dateset.

##Introduction
We are going to apply different but not exhaustive classification methods in the report. The $F_1$ score is used as evaluation metric, the reason is coming from the unbalanced sample size in the dataset. The Adult Income Census datast we use in this report classifies the income levels into two groups, one is "<=50K", and the other ">50K", but the ">50K" class has much smaller sample size comparing to "<=50" class, [@nielsen_2015] and many practical machine learning articles suggest that better result can always be achieved by increasing sample size. On the contrary, small sample size may hinder to achieve a desirable result. So the balanced $F_1$ score can help us evaluate both sensitivity and specificity on the target ">50K" class, this attribute is making $F_1$ a rather effective metric comparing overall accuracy, which will be showing a false high value due to the big sample size of the other class.  

The following content is broken into three sections. In the data exploration part, we will explore the data in a top-down strategy, by having a broad view of the dataset structure, and break the predictor attributes into groups according to the value classes, then look for any trend or relations across those attributes. After that, we will go into methods details, we will justify the reasons for choosing those methods used in this report. Lastly, we will evaluate the overall performances across different methods. 

##Data Exploration
The [Adult Census Income](www.kaggle.com/datasets/uciml/adult-census-income) dataset we use is from the 1994 Census bureau database. This dataset has the dimension of 32561 by 15, which has samples in rows, and measured variables in columns, so every single sample has 15 different attributes, and there are 32561 samples in total. 
```{r echo  = FALSE}
str(data)
```
The attributes are measured both quantitatively and qualitatively with respect to integer class and character class. The dependent variable "income" is what we are trying to predict, and it is in a qualitative form with only two different categories: "<=50K" and ">50K". This two classes element structure can help us narrow down the possible methods used will be either producing "YES" or "NO" results or giving the probability toward each category.
```{r echo  = FALSE}
sapply(data, function(j) class(j))
```

###Data Visualisation
Before we explore the data in further details, we want to check on the dataset for the strange and missing values. Although there is no "NA", but we can see some "?" showing in "workclass", "occupation", and "native.country" in in table below. Since they are all in the qualitative attributes, and recorded in character string, so we will decide the way to handle them, when we explore these attributes in details. 
```{r}
sum(is.na(data))
```
```{r echo = FALSE}
temp <- data[, lapply(.SD, function(j) sum(str_detect(j, "\\?")))]
data.table(vars = names(temp),
           "num of ?" = as.numeric(temp)) %>% knitr::kable()
rm(temp)
```
After the initial inspection on the dateset, we now get into more details about the dependent variable "income", and the other 14 attributes columns. For the "income", all the samples are classified into two categories: "<=50K" and ">50K". By checking the distribution across those two income levels we can see that, the income of "<=50K" is the dominant group across all the samples with the size being 3 times of the ">50K" group, and only about 24% of the sample belongs to ">50K". 
```{r}
sum(data$income == "<=50K") / sum(data$income == ">50K")
```
```{r echp = FALSE}
data <- data[, income := as.factor(income)]
plot_temp <- data[, .(pct = 100 * .N / dim(data)[1]), by = .(income)]
ggplot(plot_temp, aes(income, pct)) + 
  geom_bar(stat = "identity", width = 0.3) + 
  geom_text(aes(label = format(round(pct, 2), nsmall = 2)),
            vjust = 2, color = "white")

rm(plot_temp)
```
This unbalanced sample size can potentially make it more challenging to achieve a high accuracy on predicting ">50K" income group than the "<=50K" ones, the reason is that we have more samples to study for the "<=50K" group, but the ">50K" group is under-sampled, and if their attributes are not clustered, or lack of strong manifestation on their 14 attributes, then we can only reply on minimising the error on "<=50K" group to improve the ">50K" group accuracy. On the other hand, this can be viewed as an advantage of having only two groups of dependents. 

As mentioned in above, there are two different classes of predictors: "integer" and "character", so we will look those two classes of attributes separately. We will look into the "integer" class first. 
```{r}
cols <- names(which(sapply(data, class) == "integer"))
summary(data[, ..cols]) #check any strange value
```
The above lot of "integer" columns summary is not showing any strange numbers, except in the "capital.gain" and "capital.loss" columns have 0's for most of their statistics, it makes sense that most people are earning less than 50K/year, and so most of them won't have any investment asset in place. Apart from this, the column "fnlwgt" we need to make some clarification about its meaning in here. It is stand for "final weight", simply speaking it represents the amount of people is sharing the same attributes of the row, which that "fnlwgt" value is in. Of course the census bureau has more rigorous explanation on it, but we just want to highlight its basic meaning to avoid the ambiguity caused by its name having a "weight" in it. 
```{r}
num_fp <- data[, ..cols][, lapply(.SD, 
              function(j) (j - mean(j)) / sd(j)), .SDcols = cols]
summary(num_fp) #check potential outliers by normalising the columns
```
After we normalise all the "integer" columns, it can give us much clearer picture about how far away those outliers are. The boxplot in below illustrate our finding in further. All of these six numeric attributes have many outliers indeed, and majority of the outliers are locating in the upper extreme, except the weekly working hour and number of years education taken, the outliers in those two attributes are locating in both upper and lower extreme. By comparing the distributions between "<=50K" on the left and ">50K" on the right, the age attribute is showing a reasonable picture to show that the strong earning power's age group are a bit older than "<=50K" group, and also has lower upper bound age. Meanwhile, the distribution of years of education is also telling us that, the extra number of years we invest in study can definitely reward us a good income in later days. At the same time, the weekly working hour shows the hard work pays off in everybody's belief. Next we will look into the relations in between those numeric attributes under different income groups. 
```{r echo = FALSE}
num_fp <- num_fp[, income := data$income]

featurePlot(x = num_fp[, 1:6], y = num_fp$income, plot = "box", 
            scales = list(y = list(relation = "free"),
                          x = list(rot = 90)),
            layout = c(3, 2), auto.key = list(columns = 2))
```
By inspecting the scatter plot matrix in below, none of the pairs out of those six numeric attributes are showing any clear trend or correlations, instead most pairs are really just random clusters. And this kind of non-correlation randomness are sharing in both "<=50k" and ">50K" income groups.
```{r echo = FALSE, warning = FALSE}
trellis.par.set("fontsize", list(text = 8.5))
featurePlot(x = num_fp[, 1:6], y = num_fp$income, plot = "pairs", 
            auto.key = list(columns = 2))
```
We now turn to the character class attributes. But before we proceed, we need to decide how we are going to treat those "?" from the exploration in above. Firstly, we want to have a general idea about the how many of them are there in our dataset, because from the total number of them, we can evaluate the potential harm of having them in place, then we decide how they could be handled to fit in our algorithms to be used later. 
```{r}
#collect all the char columns for future process
char_cols <- names(data)[-which(names(data) %in% cols)][-9]

###check the missing ?
sum(data == "?") / (dim(data)[1] * dim(data)[2])
mis_locate <- data[, lapply(.SD, function(i) sum(i == "?")), 
                   .SDcols = char_cols]
data.frame(var.name = names(mis_locate),
           pct.in.data = as.numeric(100 * mis_locate / dim(data)[1])) %>% 
           knitr::kable() #total % of each col elements are "?"
```
The above figures are telling us that the "?" issue is not major, which is only 0.87% out of the whole dataset. In the column wise, we have seen the "?" only exists in three columns, and only "work class" and "occupation" have 5% of the data are recorded in "?", the "native.country" has less than 2% out of total. So those "?" values are too small to cause our dataset being sparse in whole. Also there is one good thing about the location of them, that none of the "?" is in the dependent column "income", which is to be predicted column, that means all the rows are the valid samples can be used. The other good thing is that, they are all categorical data, if we use an tree method to process them, they will not cause any problem in those kind of YES/NO logical algorithms. So we will leave them in place for now, and only treat them on an ad hoc basis.

From the following block of bar plots, we can have a visual understanding of what the major class is in each attribute, that makes a surveyed sample to be a ">50K" income earner. The way to visually decide that class is the major driver of one income group is that, there is big gap for the same class showing across the two income groups. For example, the "workclass" attribute is clearly showing that the "never worked" and "without pay" are the two major classes can help distinguish the "<=50K" from the ">50K" income group. The "education" attribute has very interesting find out, it shows that those highly educated candidates with degrees in bachelor, master, and doctor are not primarily clustered in high income group, instead there is quite a lot of them are in "<=50K" income group. That is very counter intuitive with the findings from the education years in above numeric attributes. Apart from that, the attributes allocation between the two income groups are quite similar. 
```{r echo = FALSE}
#create the character column only data frame
char_fp <- data[, ..char_cols]
char_fp <- char_fp[, lapply(.SD, as.factor)] 
char_fp <- char_fp[, income := data$income]
setDF(char_fp) #the following plot function need the data to be data frame

ggp1 <- lapply(1:3, function(j){
  ggplot(char_fp, aes(x = char_fp[,j], fill = income)) + 
    geom_bar() + scale_y_log10() + 
    theme(axis.text.x = element_text(angle = 90, size = 8)) + 
    facet_grid(income ~., scale = "free_y")
  })
ggmatrix(ggp1, nrow = 1, ncol = 3, xAxisLabels = char_cols[1:3])
```
```{r echo = FALSE}
ggp2 <- lapply(4:6, function(j){
  ggplot(char_fp, aes(x = char_fp[,j], fill = income)) + 
    geom_bar() + scale_y_log10() + 
    theme(axis.text.x = element_text(angle = 90, size = 8)) + 
    facet_grid(income ~., scale = "free_y")
})
ggmatrix(ggp2, nrow = 1, ncol = 3, xAxisLabels = char_cols[4:6])
```
```{r echo = FALSE}
ggp3 <- lapply(7:8, function(j){
  ggplot(char_fp, aes(x = char_fp[,j], fill = income)) + 
    geom_bar() + scale_y_log10() + 
    theme(axis.text.x = element_text(angle = 90, size = 8)) + 
    facet_grid(income ~., scale = "free_y")
})
ggmatrix(ggp3, nrow = 1, ncol = 2, xAxisLabels = char_cols[7:8])
```
So after going through those attributes visually, at least by human eyes, we can not set any model up to classify the two income groups effectively. But at least, these visual information can help us choose the possible working methods, which could be effective to handle those not strongly featured data. 

```{r echo = FALSE}
#clean out the memory
rm(ggp1, ggp2, ggp3, char_fp, mis_locate, num_fp)
```

##Methods Details
After the above visual understanding about the dataset, straightaway we can narrow down the methods selections down to trees, because the tree family can effectively handle different classes of variables, no matter they are numbers or character strings. So we will use a single tree method first, then use a forest method to compare how much better the result can be. But before everything we will utilise the dominance class phenomenon in the sample dependents "income" to make predictions based on purely guessing, we can use this dummy algorithm result as our benchmark to measure the other attending methods performances. 

###0. Guessing
We are breaking the original dataset into a 8/2 fraction of train/test set first, and use the prevalence of the dominant group of "<=50" in the training set "income" column to toss a highly biased coin with faces of "<=50K" and ">50K", we will toss the coin for the times equal the number of samples we are guessing for in the test set. 
```{r eval = FALSE}
#break data into 8/2 train/test set
set.seed(1, sample.kind = "Rounding")
ind <- createDataPartition(1:dim(data)[1], times = 1, list = FALSE, p = 0.2)
train <- data[-ind, ]
test <- data[ind, ]

#0. guessing
#chance of seeing <=50k in training set
prev <- sum(train$income == "<=50K") / dim(train)[1] 

#use the <=50k chance to toss coin for dim(test)[1] number of times
set.seed(10, sample.kind = "Rounding")
pred_guess <- sample(c("<=50K", ">50K"), dim(test)[1], replace = TRUE, 
                     prob = c(prev, 1 - prev)) 
gauge_guess <- confusionMatrix(as.factor(pred_guess), test$income, 
                               mode = "prec_recall",
                               positive = ">50K")
```
```{r echo = FALSE}
print(gauge_guess)
gauge_guess$byClass["F1"]
```
We get a $F_1$ score of 0.246413 by tossing the coin, and this will be our benchmark of performance, we are expecting all the proper algorithms in below to have a substantial improvement above this baseline. By looking at the above performance stats from guessing, it proves that the accuracy is very biased performance metric has 0.629, which is a consequence of dominant dependent in test set. The kappa value of 4e-04 is telling us this accuracy is nothing different from coincidence. If we guess everyone is "<=50K", we can even get a much higher accuracy of 0.7571012. 
```{r}
sum(test$income == "<=50K") / dim(test)[1]
```

###1. One tree
Now we are turning to train and test with property algorithms. We will start with a single CART tree by using the "rpart" package. According to the package vignettes, there are a list of parameters can be tuned to affect the trained model performance in the test set, but the most effective one is complexity parameter "cp" [@r-rpart]. It is the parameter to decide how the tree is going to be pruned from the default tree, which is grown by the set of system default parameters. the "cp" value is meant to be in the range of [0, 1], when "cp" = 1 then the tree could be a stick with no branch at all, which can be said to be very under trained. When the "cp" = 0 could cause the tree to have the number of branches to be the total number of elements in the dataset - 1, alternatively it is to be said the model is very over trained. We will just use the default value for minimum number of samples to cause further branch split, and also for the minimum samples to be in a terminal node. The "raprt" package can also handle missing values by setting some rules with a set of surrogate parameters, although we have some small fraction of predictor variables are missing and recorded in "?", we will arbitrarily treat them as a valid value class, since they are all recorded as character string inside the character class columns 
```{r eval = FALSE}
registerDoParallel(cores = 4) 
#caret can parallel the process to speed up the training

set.seed(2, sample.kind = "Rounding")
ind_cv <- createFolds(1:dim(train)[1], k = 5, returnTrain = TRUE)

#define the cross validation index can make the result to be reproducible
treecontrol <- trainControl(method = "cv", index = ind_cv)
fit_tree <- train(income ~ ., data = train, method = "rpart",
              trControl = treecontrol, 
              tuneGrid = data.frame(cp = seq(0, 0.1, length.out = 10)))
```
```{r echo = FALSE}
plot(fit_tree)
```
```{r echo = FALSE}
fit_tree$finalModel
```
The cross validation results suggest when "cp" = 0.01111111, the model can provide us a favourable result. From the printing of the final mode, it shows our final tree has six leaf nodes or terminal nodes. In those leafs we can see that the majority of the classifiers are the capital gain and loss, which just coincide with our visual findings in above. 
```{r eval = FALSE}
pred_tree <- predict(fit_tree, test)
gauge_tree <- confusionMatrix(pred_tree, test$income, 
                              mode = "prec_recall",
                              positive = ">50K")
```
```{r echo = FALSE}
print(gauge_tree)
gauge_tree$byClass["F1"]
```
The final $F_1$ score of our single tree is 0.6372981, which is very encouraging comparing to 0.246413 of the guessing benchmark, that is 158% improvement. And the kappa ratio 0.5445 is telling us our higher over all accuracy is not by chance. 

###2. Forest
Since we have tried the tree method, then the forest method can not be ignored in the tree family. As stated in Prof. Irizarry's text, random forests can improve the single decision tree performance by bagging on multiple smaller trees [@irizarry_2022]. The logic behind the bagging is that, instead of growing one tree out of all the available 14 predictors, we randomly pick "mtry" number of predictors to grow "ntree" number of small trees, the size of the trees is defined by the "nodesize", with smaller trees having larger "nodesize". So with the central idea of bagging, we would avoid to have the situation we faced in above single tree, where very few predictors "capital gain" and "capital loss" dominate the decision rules. This could be a sign of under training. And there also could be case of over training in a tree, if the rule of branch splitting is too granular, the bagging method can also accommodate this situation. 

As we described in above, we will tune our "randomForest" with three major parameters: "mtry", "nodesize", "ntree" sequentially. 
```{r eval = FALSE}
#tune the mtry 1st to decide how many predictors for each trees
#same cv index from the tree
rfcontrol <- trainControl(method = "cv", index = ind_cv)
tune_forest <- train(income ~., data = train, method = "rf",
                    trControl = rfcontrol, 
                    tuneGrid = data.frame(mtry = seq(2, 14, 2)))
```
```{r echo = FALSE}
plot(tune_forest)
```
```{r echo = FALSE}
tune_forest
```
```{r eval = FALSE}
#tune the nodesize to decide how big the leaf size
nodesize <- seq(1, 20, 2)
tune_nds <- sapply(nodesize, function(nd){
  train(income ~., data = train, method = "rf",
        trControl = rfcontrol,
        tuneGrid = data.frame(mtry = best_mtry),
        nodesize = nd)$results$Accuracy
})
```
```{r echo = FALSE}
qplot(nodesize, tune_nds, geom = c("point", "line"))
```
```{r eval = FALSE}
#tune the number of trees in the forest
ntrees <- seq(10, 200, 10)
tune_ntree <- sapply(ntrees, function(nt){
  train(income ~., data = train, method = "rf",
        trControl = rfcontrol,
        tuneGrid = data.frame(mtry = best_mtry),
        nodesize = best_node,
        ntree = nt)$results$Accuracy})
```
```{r echo = FALSE}
qplot(ntrees, tune_ntree, geom = c("point", "line"))
```
The above turning process suggest that the optimised forest is composed by "ntrees" = 180 trees, the minimum leaf size "nodesize" = 3, which means the trees in the forest are quite large, and the number of predictors "mtry" we use to grow each tree is equal to 12, almost include all the 14 available ones. Then we check on the test set performance in below. 
```{r eval = FALSE}
fit_forest <- train(income ~., data = train, method = "rf",
                     tuneGrid = data.frame(mtry = best_mtry),
                     nodesize = best_node,
                     ntree = best_ntree)

pred_forest <- predict(fit_forest, test)
gauge_forest <- confusionMatrix(pred_forest, test$income, 
                                mode = "prec_recall",
                                positive = ">50K")
```
```{r echo = FALSE}
print(gauge_forest)
gauge_forest$byClass["F1"]
```
The final $F_1$ score 0.6908837 does meet our initial expectation of achieving a better performance than the single tree in before, although the improvement is very limited for just over 8%. In next we will jump out of trees family, to try another 